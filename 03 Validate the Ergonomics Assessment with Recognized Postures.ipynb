{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf  # Version 1.0.0 (some previous versions are used in past commits)\n",
    "from sklearn import metrics\n",
    "import os\n",
    "from numpy import genfromtxt\n",
    "from scipy import stats\n",
    "from sklearn import preprocessing\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import Callback\n",
    "from keras.models import load_model\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\n",
    "import keras\n",
    "from keras.layers import Dense, Flatten, Reshape,Dropout\n",
    "from keras.layers import Conv2D, MaxPooling2D, LSTM\n",
    "from keras.models import Sequential\n",
    "from sklearn.model_selection import train_test_split\n",
    "import timeit #package for recording the model running time\n",
    "import time\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense, Conv2D, Conv3D, MaxPooling3D, Reshape, BatchNormalization, MaxPooling2D\n",
    "from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import metrics\n",
    "from keras.optimizers import Adam\n",
    "from keras.optimizers import SGD  \n",
    "from keras import backend as K\n",
    "from sklearn.metrics import fbeta_score\n",
    "from sklearn.model_selection import KFold,StratifiedKFold,ShuffleSplit,StratifiedShuffleSplit\n",
    "# Packages & Setup\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import  preprocessing\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report,f1_score,accuracy_score\n",
    "import timeit\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def win_seg(data,windowsize,overlap):#function for overlap segmentation\n",
    "    length=int((data.shape[0]*data.shape[1]-windowsize)/(windowsize*overlap)+1)\n",
    "    newdata=np.empty((length,windowsize, data.shape[2],1))\n",
    "    data_dim=data.shape[2]\n",
    "    layers=data.shape[3]\n",
    "    data=data.reshape(-1,data_dim,layers)\n",
    "    for i in range(0,length) :\n",
    "        start=int(i*windowsize*overlap)\n",
    "        end=int(start+windowsize)\n",
    "        newdata[i]=data[start:end]\n",
    "    return newdata\n",
    "def lab_vote(data,windowsize):\n",
    "    y_data=data.reshape(-1,windowsize,1,1)\n",
    "    y_data=win_seg(y_data,windowsize,0.5)\n",
    "    y_data=y_data.reshape(y_data.shape[0],y_data.shape[1],y_data.shape[2])\n",
    "    y_data=stats.mode(y_data,axis=1)\n",
    "    y_data=y_data.mode\n",
    "    y_data=y_data.reshape(-1,1)\n",
    "    y_data=np.float64(keras.utils.to_categorical(y_data))\n",
    "    return y_data\n",
    "def lab_vote_cat(data,windowsize): # non one-hot coding\n",
    "    y_data=data.reshape(-1,windowsize,1,1)\n",
    "    y_data=win_seg(y_data,windowsize,0.5)\n",
    "    y_data=y_data.reshape(y_data.shape[0],y_data.shape[1],y_data.shape[2])\n",
    "    y_data=stats.mode(y_data,axis=1)\n",
    "    y_data=y_data.mode\n",
    "    y_data=y_data.reshape(-1,1)\n",
    "    return y_data\n",
    "def write_csv(data):\n",
    "    a = np.asarray(data)\n",
    "    a.tofile('check.csv',sep=',',format='%10.5f')\n",
    "def average(lst): \n",
    "    a = np.array(lst)\n",
    "    return np.mean(a)\n",
    "class TimeHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.times = []\n",
    "    def on_epoch_begin(self, batch, logs={}):\n",
    "        self.epoch_time_start = time.time()\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.times.append(time.time() - self.epoch_time_start)\n",
    "def f1(y_true, y_pred):\n",
    "    y_pred = K.round(y_pred)\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    # tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return K.mean(f1)\n",
    "def yindex(y): #all contineous lables treated as one class\n",
    "    y_index=[0]\n",
    "    label=0\n",
    "    for i in range(1,len(y)):\n",
    "        if y[i]==y[i-1]:\n",
    "            y_index.append(label)\n",
    "        else:\n",
    "            label+=1\n",
    "            y_index.append(label)\n",
    "    return y_index\n",
    "def continue_split(x,y,y_index): #get the last 10% of every contineous movement\n",
    "    skf = StratifiedKFold(n_splits=10,shuffle=False)\n",
    "    counter=0\n",
    "    for train_index, test_index in skf.split(x, y_index):\n",
    "        if counter<9:\n",
    "            counter+=1\n",
    "            continue\n",
    "    x_data_a, x_data_b=x[train_index],x[test_index]\n",
    "    y_data_a, y_data_b=y[train_index],y[test_index]\n",
    "    return x_data_a, x_data_b, y_data_a, y_data_b,train_index,test_index\n",
    "\n",
    "def quasiEVA(lable_array,threshold,result):\n",
    "    j=0\n",
    "    c=list(np.unique(lable_array)) #total number of posture classes\n",
    "    c=[int(i) for i in c]\n",
    "    rows=len(lable_array) #total number of postures\n",
    "    #count consequtive postures\n",
    "    count=np.zeros(shape=(rows,2)) #buffer to store all cases of contineous postures\n",
    "    count[0,1]=1\n",
    "    for i in range(rows-1):\n",
    "        if lable_array[i]==lable_array[i+1]:\n",
    "            count[j,1]+=1\n",
    "            count[j,0]=lable_array[i]\n",
    "        else:\n",
    "            j+=1\n",
    "            count[j,0]=lable_array[i+1]\n",
    "            count[j,1]=1\n",
    "    count=count[count[:,1]>0] #get ride of empty rows\n",
    "    #count holding time\n",
    "    HT=np.zeros(shape=(len(count),1))\n",
    "    HT[:,0]=(count[:,1]+1)*0.5\n",
    "    count=np.append(count,HT,1)\n",
    "    for ind,val in enumerate(c):\n",
    "            count_red=count[count[:,0]==val] #Get all the contineous occurance of posture i\n",
    "            count_red_result=count_red[count_red[:,2]>threshold[ind]] # filtering out all the MHT above threshold\n",
    "            result.iloc[[ind],[1]]=len(count_red_result[:,1]) #total count of breaching MHT\n",
    "            result.iloc[[ind],[2]]=sum(count_red_result[:,2]) #total duration of breaching MHT\n",
    "            result.iloc[[ind],[3]]=(len(count_red[:,1])/sum(count[:,2]))*60 #frequencies of activity i occurance in 1 minute\n",
    "            result.iloc[[ind],[4]]=sum(count_red[:,1])/sum(count[:,1]) #proportion of activity i\n",
    "            result.iloc[[ind],[5]]=max(count_red[:,2]) #max time of holding\n",
    "    return result, count,count_red"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply both DNN and ML Models for Posture Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the training and testing data, using R3 as example\n",
    "buffer = np.float64(preprocessing.scale(genfromtxt('S3_X.csv', delimiter=',')))\n",
    "x_data=buffer.reshape(-1,40,30,1)\n",
    "x_data=win_seg(x_data,40,0.5) # data segmentation with 0.5 overlap\n",
    "#majority vote on training label\n",
    "buffer = np.float64(genfromtxt('S3_Y.csv', delimiter=','))-1 #0 based index\n",
    "y_data=lab_vote(buffer,40)\n",
    "#fix the dimension mismatch\n",
    "if y_data.shape[1]<7:\n",
    "    zeros=np.zeros((y_data.shape[0],7-y_data.shape[1]))\n",
    "    y_data=np.hstack((y_data,zeros))\n",
    "y_data2=lab_vote_cat(buffer,40) # for stratification purposes\n",
    "y_index=yindex(y_data2)\n",
    "x_data_a, x_data_b, y_data_a, y_data_b,train_index,test_index=continue_split(x_data,y_data,y_index)\n",
    "\n",
    "# Load the ML full feature and construct the one with selected generalized features\n",
    "# Export the best features\n",
    "sel_features=pd.DataFrame()\n",
    "fullfeatures=pd.read_csv(\"S3_Full Features.csv\")\n",
    "names=list(fullfeatures.columns.values)[1:]\n",
    "sel_name_list=pd.read_csv(\"Generalized ML_SelectetedFeaturesNames.csv\")\n",
    "sel_name_list=list(sel_name_list[\"feature names\"])\n",
    "for index, val in enumerate(sel_name_list):\n",
    "    if val:\n",
    "        sel_features=pd.concat([sel_features,fullfeatures.iloc[:,index+1]],axis=1)\n",
    "\n",
    "sel_features.to_csv(\"S3_Selecteted Features.csv\")\n",
    "# Data Prepration\n",
    "X_ML_train, X_ML_test, X_ML_data,y_ML_train, y_ML_test, y_ML_data=preparation(\"S3_Selecteted Features.csv\")\n",
    "X_ML_train=X_ML_data[train_index]\n",
    "y_ML_train=y_ML_data[train_index]\n",
    "X_ML_test=X_ML_data[test_index]\n",
    "y_ML_test=y_ML_data[test_index]\n",
    "# Get the ML prediction Result\n",
    "svm=SVC(gamma='auto',random_state=42)\n",
    "svm.fit(X_ML_train,y_ML_train)\n",
    "svm_pre=pd.DataFrame(data=svm.predict(X_ML_test))\n",
    "acc=accuracy_score(y_ML_test, svm_pre)\n",
    "f1=f1_score(y_ML_test, svm_pre,average='macro')\n",
    "ML_Result,ML_Pre=TrainModels(X_ML_train, X_ML_test, y_ML_train, y_ML_test)\n",
    "ML_Result.to_csv(\"S3_ML_Performance.csv\")\n",
    "ML_Pre.to_csv(\"S3_ML_SVM_Recognition.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DNN Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#incremental training on first half\n",
    "#Model Evaluation Metrics\n",
    "acc_score=list()\n",
    "f_score=list()\n",
    "eopch_time_record=list()\n",
    "oper_time_record=list()\n",
    "i=0\n",
    "\n",
    "#Reload the trained model on rest subjects\n",
    "newmodel=load_model(\"rest.h5\", custom_objects={'f1': f1})\n",
    "#split the train data into training (training the model) and validation (tuning hypeparameters) by 8:2\n",
    "X_training, X_validation, y_training, y_validation = train_test_split(x_data_a, y_data_a, test_size=0.20)\n",
    "#Setup Model Parameters\n",
    "data_dim = x_data_a.shape[2] #y of figure\n",
    "timesteps = x_data_a.shape[1] #x of figure\n",
    "num_classes = y_data_a.shape[1]\n",
    "batchsize=300\n",
    "epcoh=300\n",
    "checkpointer = ModelCheckpoint(filepath=\"S3.h5\", monitor='val_f1',verbose=1, mode='max', save_best_only=True)\n",
    "time_callback = TimeHistory() #record the model training time for each epoch\n",
    "callbacks_list = [checkpointer,time_callback]\n",
    "newmodel.compile(loss=keras.losses.categorical_crossentropy,\n",
    "          optimizer=keras.optimizers.Adam(lr=0.001),metrics=['accuracy',f1])\n",
    "train_history=newmodel.fit(X_training, y_training,\n",
    "          batch_size=batchsize, epochs=epcoh,callbacks=callbacks_list,\n",
    "          validation_data=(X_validation, y_validation))\n",
    "CNN_LSTM_model=load_model(\"S3.h5\", custom_objects={'f1': f1})\n",
    "\n",
    "#check the model test result\n",
    "y_pred=CNN_LSTM_model.predict(x_data_b)\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "Y_test=np.argmax(y_data_b, axis=1)\n",
    "acc_score.append(accuracy_score(Y_test, y_pred)) # Evaluation of accuracy\n",
    "f_score.append(f1_score(Y_test, y_pred,average='macro')) # Evaluation of F1 score\n",
    "performance=pd.DataFrame(columns=['Acc_score','Macro_Fscore'])\n",
    "performance['Acc_score']=acc_score\n",
    "performance['Macro_Fscore']=f_score\n",
    "performance.to_csv(\"LeaveOneOutOnS3.csv\")\n",
    "recognition_results=pd.DataFrame(columns=['Prediction','Actual'])\n",
    "recognition_results['Prediction']=y_pred\n",
    "recognition_results['Actual']=Y_test\n",
    "recognition_results.to_csv('LeaveOneOutOnS3_Recognition.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare Ergonomics Assessment Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Activitie Recognition Result from csv as numpy array, using postures recognized from incremental DNN model as example\n",
    "act=genfromtxt('LeaveOneOutOnS3_Recognition.csv', delimiter=',',skip_header =1) # change the file name to get postures from Ground-Truth (G), Incremental Model (I),and Personalized Model\n",
    "act=act[:,-1]#last column is actual\n",
    "# Define Threshold for Activities\n",
    "threshold= np.array([3,3,3,30]) # Safety Threshold in seconds\n",
    "lab_names= np.array([\"BT\",\"KN\",\"SQ\",\"ST\"])\n",
    "columns=['Activities','Count of MHT','Total Duration of MHT','Frequency of Exposture in 1 Minute','Proportion of Activities','Maximum Holding Time']\n",
    "index=range(len(lab_names))\n",
    "result = pd.DataFrame(index=index,columns=columns)\n",
    "result.iloc[:,0]=lab_names\n",
    "result = result.fillna(0) # with 0s rather than NaNs\n",
    "# Get the Quasi-Realtime Evaluation Result\n",
    "eva_result,count,count_red=quasiEVA(act,threshold,result)\n",
    "eva_result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
