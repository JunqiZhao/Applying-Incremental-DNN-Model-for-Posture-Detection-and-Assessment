{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#package preparation\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf  # Version 1.0.0 (some previous versions are used in past commits)\n",
    "from sklearn import metrics\n",
    "import os\n",
    "from numpy import genfromtxt\n",
    "from scipy import stats\n",
    "from sklearn import preprocessing\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import Callback\n",
    "from keras.models import load_model\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\n",
    "import keras\n",
    "from keras.layers import Dense, Flatten, Reshape,Dropout\n",
    "from keras.layers import Conv2D, MaxPooling2D, LSTM\n",
    "from keras.models import Sequential\n",
    "from sklearn.model_selection import train_test_split\n",
    "import timeit #package for recording the model running time\n",
    "import time\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense, Conv2D, Conv3D, MaxPooling3D, Reshape, BatchNormalization, MaxPooling2D\n",
    "from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import metrics\n",
    "from keras.optimizers import Adam \n",
    "from keras import backend as K\n",
    "from sklearn.metrics import fbeta_score\n",
    "from sklearn.model_selection import KFold,StratifiedKFold,ShuffleSplit,StratifiedShuffleSplit\n",
    "# Packages & Setup\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import  preprocessing\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report,f1_score,accuracy_score\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function preparation\n",
    "def win_seg(data,windowsize,overlap):#function for overlap segmentation\n",
    "    length=int((data.shape[0]*data.shape[1]-windowsize)/(windowsize*overlap)+1)\n",
    "    newdata=np.empty((length,windowsize, data.shape[2],1))\n",
    "    data_dim=data.shape[2]\n",
    "    layers=data.shape[3]\n",
    "    data=data.reshape(-1,data_dim,layers)\n",
    "    for i in range(0,length) :\n",
    "        start=int(i*windowsize*overlap)\n",
    "        end=int(start+windowsize)\n",
    "        newdata[i]=data[start:end]\n",
    "    return newdata\n",
    "def lab_vote(data,windowsize):\n",
    "    y_data=data.reshape(-1,windowsize,1,1)\n",
    "    y_data=win_seg(y_data,windowsize,0.5)\n",
    "    y_data=y_data.reshape(y_data.shape[0],y_data.shape[1],y_data.shape[2])\n",
    "    y_data=stats.mode(y_data,axis=1)\n",
    "    y_data=y_data.mode\n",
    "    y_data=y_data.reshape(-1,1)\n",
    "    y_data=np.float64(keras.utils.to_categorical(y_data))\n",
    "    return y_data\n",
    "def lab_vote_cat(data,windowsize): # non one-hot coding\n",
    "    y_data=data.reshape(-1,windowsize,1,1)\n",
    "    y_data=win_seg(y_data,windowsize,0.5)\n",
    "    y_data=y_data.reshape(y_data.shape[0],y_data.shape[1],y_data.shape[2])\n",
    "    y_data=stats.mode(y_data,axis=1)\n",
    "    y_data=y_data.mode\n",
    "    y_data=y_data.reshape(-1,1)\n",
    "    return y_data\n",
    "def write_csv(data):\n",
    "    a = np.asarray(data)\n",
    "    a.tofile('check.csv',sep=',',format='%10.5f')\n",
    "def average(lst): \n",
    "    a = np.array(lst)\n",
    "    return np.mean(a)\n",
    "class TimeHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.times = []\n",
    "    def on_epoch_begin(self, batch, logs={}):\n",
    "        self.epoch_time_start = time.time()\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.times.append(time.time() - self.epoch_time_start)\n",
    "def f1(y_true, y_pred):\n",
    "    y_pred = K.round(y_pred)\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    # tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return K.mean(f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-to-One Incremental Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Developing Incremental and Evaluating Incremental Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading data for next subject, use S4 as an example\n",
    "buffer = np.float64(preprocessing.scale(genfromtxt('S4_X.csv', delimiter=',')))\n",
    "x_data=buffer.reshape(-1,40,30,1)\n",
    "x_data=win_seg(x_data,40,0.5) # data segmentation with 0.5 overlap\n",
    "#majority vote on training label\n",
    "buffer = np.float64(genfromtxt('S4_Y.csv', delimiter=','))-1 #0 based index\n",
    "y_data=lab_vote(buffer,40)\n",
    "#fix the dimension mismatch\n",
    "if y_data.shape[1]<7:\n",
    "    zeros=np.zeros((y_data.shape[0],7-y_data.shape[1]))\n",
    "    y_data=np.hstack((y_data,zeros))\n",
    "y_data2=lab_vote_cat(buffer,40) # for stratification purposes\n",
    "#Stratified random shuffle\n",
    "sf = ShuffleSplit(n_splits=3, test_size=0.1, random_state=42)\n",
    "sss=StratifiedShuffleSplit(n_splits=5, test_size=0.1, random_state=42)\n",
    "\n",
    "#Model Evaluation Metrics\n",
    "acc_score=list()\n",
    "f_score=list()\n",
    "eopch_time_record=list()\n",
    "oper_time_record=list()\n",
    "i=0\n",
    "for train_index, test_index in sss.split(x_data,y_data):\n",
    "    #Reload the trained model for S3\n",
    "    newmodel=load_model(\"S3%s.h5\" % i, custom_objects={'f1': f1})\n",
    "    X_train, X_test = x_data[train_index], x_data[test_index]\n",
    "    y_train, y_test = y_data[train_index], y_data[test_index]\n",
    "    #split the train data into training (training the model) and validation (tuning hypeparameters) by 8:2\n",
    "    X_training, X_validation, y_training, y_validation = train_test_split(X_train, y_train, test_size=0.20)\n",
    "    #Setup Model Parameters\n",
    "    data_dim = X_train.shape[2] #y of figure\n",
    "    timesteps = X_train.shape[1] #x of figure\n",
    "    num_classes = y_train.shape[1]\n",
    "    batchsize=300\n",
    "    epcoh=300\n",
    "    checkpointer = ModelCheckpoint(filepath=\"S4%s.h5\" % i, monitor='val_f1',verbose=1, mode='max', save_best_only=True)\n",
    "    time_callback = TimeHistory() #record the model training time for each epoch\n",
    "    callbacks_list = [checkpointer,time_callback]\n",
    "    newmodel.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adam(lr=0.001),metrics=['accuracy',f1])\n",
    "    train_history=newmodel.fit(X_training, y_training,\n",
    "              batch_size=batchsize, epochs=epcoh,callbacks=callbacks_list,\n",
    "              validation_data=(X_validation, y_validation))\n",
    "    eopch_time=time_callback.times\n",
    "    eopch_time_record.append(eopch_time) #record the traing time of each epoch\n",
    "    CNN_LSTM_model=load_model(\"S4%s.h5\" % i, custom_objects={'f1': f1})\n",
    "    #model operation and timing\n",
    "    start=timeit.default_timer()\n",
    "    y_pred=CNN_LSTM_model.predict(X_test)\n",
    "    stop=timeit.default_timer()\n",
    "    oper_time=stop-start\n",
    "    oper_time_record.append(oper_time)\n",
    "    #check the model test result\n",
    "    y_pred=CNN_LSTM_model.predict(X_test)\n",
    "    y_pred = np.argmax(y_pred, axis=1)\n",
    "    Y_test=np.argmax(y_test, axis=1)\n",
    "    acc_score.append(accuracy_score(Y_test, y_pred)) # Evaluation of accuracy\n",
    "    f_score.append(f1_score(Y_test, y_pred,average='macro')) # Evaluation of F1 score\n",
    "    print(\"This is the\", i+1,  \"out of \",5, \"Shuffle\")\n",
    "    i+=1\n",
    "# Record incremetnal performance\n",
    "performance=pd.DataFrame(columns=['Acc_score','Macro_Fscore','Average_Epoch','Average_Run'])\n",
    "performance['Acc_score']=acc_score\n",
    "performance['Macro_Fscore']=f_score\n",
    "performance['Average_Epoch']=average(eopch_time_record)\n",
    "performance['Average_Run']=average(oper_time_record)\n",
    "performance.to_csv(\"Incremental Performance on S4.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Forgetting Efffect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading data of learned subject\n",
    "buffer = np.float64(preprocessing.scale(genfromtxt('S3_X.csv', delimiter=',')))\n",
    "x_data=buffer.reshape(-1,40,30,1)\n",
    "x_data=win_seg(x_data,40,0.5) # data segmentation with 0.5 overlap\n",
    "#majority vote on training label\n",
    "buffer = np.float64(genfromtxt('S3_Y.csv', delimiter=','))-1 #0 based index\n",
    "y_data=lab_vote(buffer,40)\n",
    "#fix the dimension mismatch\n",
    "if y_data.shape[1]<7:\n",
    "    zeros=np.zeros((y_data.shape[0],7-y_data.shape[1]))\n",
    "    y_data=np.hstack((y_data,zeros))\n",
    "y_data2=lab_vote_cat(buffer,40) # for stratification purposes\n",
    "#stratified random shaffle\n",
    "sf = ShuffleSplit(n_splits=3, test_size=0.1, random_state=42)\n",
    "sss=StratifiedShuffleSplit(n_splits=5, test_size=0.1, random_state=42)\n",
    "\n",
    "#Model Evaluation Metrics\n",
    "acc_score=list()\n",
    "f_score=list()\n",
    "eopch_time_record=list()\n",
    "oper_time_record=list()\n",
    "i=0\n",
    "for train_index, test_index in sss.split(x_data,y_data):\n",
    "    X_train, X_test = x_data[train_index], x_data[test_index]\n",
    "    y_train, y_test = y_data[train_index], y_data[test_index]\n",
    "    #split the train data into training (training the model) and validation (tuning hypeparameters) by 8:2\n",
    "    X_training, X_validation, y_training, y_validation = train_test_split(X_train, y_train, test_size=0.20)\n",
    "    #Setup Model Parameters\n",
    "    data_dim = X_train.shape[2] #y of figure\n",
    "    timesteps = X_train.shape[1] #x of figure\n",
    "    num_classes = y_train.shape[1]\n",
    "    batchsize=300\n",
    "    epcoh=300\n",
    "    #Load Model\n",
    "    CNN_LSTM_model=load_model(\"S4.h5\" % i, custom_objects={'f1': f1})\n",
    "    #model operation and timing\n",
    "    start=timeit.default_timer()\n",
    "    y_pred=CNN_LSTM_model.predict(X_test)\n",
    "    stop=timeit.default_timer()\n",
    "    oper_time=stop-start\n",
    "    oper_time_record.append(oper_time)\n",
    "    #check the model test result\n",
    "    y_pred=CNN_LSTM_model.predict(X_test)\n",
    "    y_pred = np.argmax(y_pred, axis=1)\n",
    "    Y_test=np.argmax(y_test, axis=1)\n",
    "    acc_score.append(accuracy_score(Y_test, y_pred)) # Evaluation of accuracy\n",
    "    f_score.append(f1_score(Y_test, y_pred,average='macro')) # Evaluation of F1 score\n",
    "    print(\"This is the\", i+1,  \"out of \",5, \"Shuffle\")\n",
    "    i+=1\n",
    "    \n",
    "# Record Performance\n",
    "performance=pd.DataFrame(columns=['Acc_score','Macro_Fscore','Average_Epoch','Average_Run'])\n",
    "performance['Acc_score']=acc_score\n",
    "performance['Macro_Fscore']=f_score\n",
    "performance['Average_Epoch']=average(eopch_time_record)\n",
    "performance['Average_Run']=average(oper_time_record)\n",
    "performance.to_csv(\"[Evaluation of Forget on S3].csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Many-to-One Incremental Learning  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Developing Incremental and Evaluating Incremental Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading data for next subject, use S4 as an example\n",
    "buffer = np.float64(preprocessing.scale(genfromtxt('S4_X.csv', delimiter=',')))\n",
    "x_data=buffer.reshape(-1,40,30,1)\n",
    "x_data=win_seg(x_data,40,0.5) # data segmentation with 0.5 overlap\n",
    "#majority vote on training label\n",
    "buffer = np.float64(genfromtxt('S4_Y.csv', delimiter=','))-1 #0 based index\n",
    "y_data=lab_vote(buffer,40)\n",
    "#fix the dimension mismatch\n",
    "if y_data.shape[1]<7:\n",
    "    zeros=np.zeros((y_data.shape[0],7-y_data.shape[1]))\n",
    "    y_data=np.hstack((y_data,zeros))\n",
    "y_data2=lab_vote_cat(buffer,40) # for stratification purposes\n",
    "#Stratified random shuffle\n",
    "sf = ShuffleSplit(n_splits=3, test_size=0.1, random_state=42)\n",
    "sss=StratifiedShuffleSplit(n_splits=5, test_size=0.1, random_state=42)\n",
    "\n",
    "#Model Evaluation Metrics\n",
    "acc_score=list()\n",
    "f_score=list()\n",
    "eopch_time_record=list()\n",
    "oper_time_record=list()\n",
    "i=0\n",
    "for train_index, test_index in sss.split(x_data,y_data):\n",
    "    #Reload the trained model for rest subjects\n",
    "    newmodel=load_model(\"rest%s.h5\" % i, custom_objects={'f1': f1})\n",
    "    X_train, X_test = x_data[train_index], x_data[test_index]\n",
    "    y_train, y_test = y_data[train_index], y_data[test_index]\n",
    "    #split the train data into training (training the model) and validation (tuning hypeparameters) by 8:2\n",
    "    X_training, X_validation, y_training, y_validation = train_test_split(X_train, y_train, test_size=0.20)\n",
    "    #Setup Model Parameters\n",
    "    data_dim = X_train.shape[2] #y of figure\n",
    "    timesteps = X_train.shape[1] #x of figure\n",
    "    num_classes = y_train.shape[1]\n",
    "    batchsize=300\n",
    "    epcoh=300\n",
    "    checkpointer = ModelCheckpoint(filepath=\"S4%s.h5\" % i, monitor='val_f1',verbose=1, mode='max', save_best_only=True)\n",
    "    time_callback = TimeHistory() #record the model training time for each epoch\n",
    "    callbacks_list = [checkpointer,time_callback]\n",
    "    newmodel.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adam(lr=0.001),metrics=['accuracy',f1])\n",
    "    train_history=newmodel.fit(X_training, y_training,\n",
    "              batch_size=batchsize, epochs=epcoh,callbacks=callbacks_list,\n",
    "              validation_data=(X_validation, y_validation))\n",
    "    eopch_time=time_callback.times\n",
    "    eopch_time_record.append(eopch_time) #record the traing time of each epoch\n",
    "    CNN_LSTM_model=load_model(\"S4%s.h5\" % i, custom_objects={'f1': f1})\n",
    "    #model operation and timing\n",
    "    start=timeit.default_timer()\n",
    "    y_pred=CNN_LSTM_model.predict(X_test)\n",
    "    stop=timeit.default_timer()\n",
    "    oper_time=stop-start\n",
    "    oper_time_record.append(oper_time)\n",
    "    #check the model test result\n",
    "    y_pred=CNN_LSTM_model.predict(X_test)\n",
    "    y_pred = np.argmax(y_pred, axis=1)\n",
    "    Y_test=np.argmax(y_test, axis=1)\n",
    "    acc_score.append(accuracy_score(Y_test, y_pred)) # Evaluation of accuracy\n",
    "    f_score.append(f1_score(Y_test, y_pred,average='macro')) # Evaluation of F1 score\n",
    "    print(\"This is the\", i+1,  \"out of \",5, \"Shuffle\")\n",
    "    i+=1\n",
    "# Record incremetnal performance\n",
    "performance=pd.DataFrame(columns=['Acc_score','Macro_Fscore','Average_Epoch','Average_Run'])\n",
    "performance['Acc_score']=acc_score\n",
    "performance['Macro_Fscore']=f_score\n",
    "performance['Average_Epoch']=average(eopch_time_record)\n",
    "performance['Average_Run']=average(oper_time_record)\n",
    "performance.to_csv(\"Incremental Performance on S4.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Forgetting Efffect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading data of learned (rest) subjects\n",
    "buffer = np.float64(preprocessing.scale(genfromtxt('rest_X.csv', delimiter=',')))\n",
    "x_data=buffer.reshape(-1,40,30,1)\n",
    "x_data=win_seg(x_data,40,0.5) # data segmentation with 0.5 overlap\n",
    "#majority vote on training label\n",
    "buffer = np.float64(genfromtxt('rest_Y.csv', delimiter=','))-1 #0 based index\n",
    "y_data=lab_vote(buffer,40)\n",
    "#fix the dimension mismatch\n",
    "if y_data.shape[1]<7:\n",
    "    zeros=np.zeros((y_data.shape[0],7-y_data.shape[1]))\n",
    "    y_data=np.hstack((y_data,zeros))\n",
    "y_data2=lab_vote_cat(buffer,40) # for stratification purposes\n",
    "#stratified random shaffle\n",
    "sf = ShuffleSplit(n_splits=3, test_size=0.1, random_state=42)\n",
    "sss=StratifiedShuffleSplit(n_splits=5, test_size=0.1, random_state=42)\n",
    "\n",
    "#Model Evaluation Metrics\n",
    "acc_score=list()\n",
    "f_score=list()\n",
    "eopch_time_record=list()\n",
    "oper_time_record=list()\n",
    "i=0\n",
    "for train_index, test_index in sss.split(x_data,y_data):\n",
    "    X_train, X_test = x_data[train_index], x_data[test_index]\n",
    "    y_train, y_test = y_data[train_index], y_data[test_index]\n",
    "    #split the train data into training (training the model) and validation (tuning hypeparameters) by 8:2\n",
    "    X_training, X_validation, y_training, y_validation = train_test_split(X_train, y_train, test_size=0.20)\n",
    "    #Setup Model Parameters\n",
    "    data_dim = X_train.shape[2] #y of figure\n",
    "    timesteps = X_train.shape[1] #x of figure\n",
    "    num_classes = y_train.shape[1]\n",
    "    batchsize=300\n",
    "    epcoh=300\n",
    "    #Load the Trained Model\n",
    "    CNN_LSTM_model=load_model(\"S4.h5\" % i, custom_objects={'f1': f1})\n",
    "    #model operation and timing\n",
    "    start=timeit.default_timer()\n",
    "    y_pred=CNN_LSTM_model.predict(X_test)\n",
    "    stop=timeit.default_timer()\n",
    "    oper_time=stop-start\n",
    "    oper_time_record.append(oper_time)\n",
    "    #check the model test result\n",
    "    y_pred=CNN_LSTM_model.predict(X_test)\n",
    "    y_pred = np.argmax(y_pred, axis=1)\n",
    "    Y_test=np.argmax(y_test, axis=1)\n",
    "    acc_score.append(accuracy_score(Y_test, y_pred)) # Evaluation of accuracy\n",
    "    f_score.append(f1_score(Y_test, y_pred,average='macro')) # Evaluation of F1 score\n",
    "    print(\"This is the\", i+1,  \"out of \",5, \"Shuffle\")\n",
    "    i+=1\n",
    "    \n",
    "# Record Performance\n",
    "performance=pd.DataFrame(columns=['Acc_score','Macro_Fscore','Average_Epoch','Average_Run'])\n",
    "performance['Acc_score']=acc_score\n",
    "performance['Macro_Fscore']=f_score\n",
    "performance['Average_Epoch']=average(eopch_time_record)\n",
    "performance['Average_Run']=average(oper_time_record)\n",
    "performance.to_csv(\"[Evaluation of Forget on Rest].csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
